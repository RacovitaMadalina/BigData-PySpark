{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7th Laboratory - 1st exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and execute a **Spark job in pySpark** to compute the **frequency of the words in Shakespeare’s\n",
    "works (the wordcount problem)**. Use the data files from the Cloudera virtual machine (copy them\n",
    "to your OS) and write the results into a directory named “counts” on the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.14:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tragedies', 'comedies', 'glossary', 'histories', 'poems']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sc.textFile('../data/shakespeare/*')\\\n",
    "             .flatMap(lambda line: re.split('\\W+', line))\\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .filter(lambda x: x[0] != \"\")\\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "\n",
    "counts.coalesce(1)\\\n",
    "      .sortBy(lambda a: a[0])\\\n",
    "      .saveAsTextFile(\"./counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._SUCCESS.crc', '.part-00000.crc', '_SUCCESS', 'part-00000']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(\"./counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 49)\n",
      "('10', 1)\n",
      "('11th', 1)\n",
      "('12th', 1)\n",
      "('1s', 1)\n",
      "('2', 48)\n",
      "('2d', 1)\n",
      "('2s', 3)\n",
      "('3', 29)\n",
      "('4', 1)\n",
      "('4d', 1)\n",
      "('5', 1)\n",
      "('5s', 1)\n",
      "('6', 1)\n",
      "('6d', 2)\n",
      "('6s', 1)\n",
      "('7', 1)\n",
      "('8', 1)\n",
      "('8d', 2)\n",
      "('9', 1)\n",
      "('A', 2027)\n",
      "('AARON', 72)\n",
      "('ABATE', 1)\n",
      "('ABATEMENT', 1)\n",
      "('ABERGAVENNY', 9)\n",
      "('ABHOR', 1)\n",
      "('ABHORSON', 18)\n",
      "('ABIDE', 1)\n",
      "('ABLE', 1)\n",
      "('ABOUT', 18)\n",
      "('ABRAHAM', 7)\n",
      "('ABRIDGEMENT', 1)\n",
      "('ABROAD', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./counts/part-00000\", \"r\")\n",
    "file_content = f.read()\n",
    "f.close()\n",
    "\n",
    "print(file_content[:390])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7th Laboratory - 2nd exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code above to **compute the frequency only for the words having at most 5 characters**.\n",
    "Show the results in the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sc.textFile('../data/shakespeare/*')\\\n",
    "             .flatMap(lambda line: re.split('\\W+', line))\\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .filter(lambda x: len(x[0]) <= 5 and len(x[0]) > 0) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \n",
    "\n",
    "counts.coalesce(1)\\\n",
    "      .sortBy(lambda a: a[0])\\\n",
    "      .saveAsTextFile(\"./counts_at_most_5_chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 49),\n",
       " ('10', 1),\n",
       " ('11th', 1),\n",
       " ('12th', 1),\n",
       " ('1s', 1),\n",
       " ('2', 48),\n",
       " ('2d', 1),\n",
       " ('2s', 3),\n",
       " ('3', 29),\n",
       " ('4', 1),\n",
       " ('4d', 1),\n",
       " ('5', 1),\n",
       " ('5s', 1),\n",
       " ('6', 1),\n",
       " ('6d', 2),\n",
       " ('6s', 1),\n",
       " ('7', 1),\n",
       " ('8', 1),\n",
       " ('8d', 2),\n",
       " ('9', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sortBy(lambda a: a[0]).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(5) PythonRDD[17] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[16] at mapPartitions at PythonRDD.scala:133 []\\n |  ShuffledRDD[15] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(5) PairwiseRDD[14] at reduceByKey at <ipython-input-7-1a6a1c0712ff>:1 []\\n    |  PythonRDD[13] at reduceByKey at <ipython-input-7-1a6a1c0712ff>:1 []\\n    |  ../data/shakespeare/* MapPartitionsRDD[12] at textFile at NativeMethodAccessorImpl.java:0 []\\n    |  ../data/shakespeare/* HadoopRDD[11] at textFile at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "print(counts.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7th Laboratory - 3rd exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and execute a Spark job in pySpark to compute the **average length of the words starting\n",
    "with the same letter** (the average word length problem) in Shakespeare’s works. Write the results\n",
    "in a directory named **“average”**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sc.textFile('../data/shakespeare/*') \\\n",
    "             .flatMap(lambda line: re.split('\\W+', line)) \\\n",
    "             .filter(lambda x: x != \"\") \\\n",
    "             .map(lambda word: (word[0], len(word)))\\\n",
    "             .groupByKey()\\\n",
    "             .map(lambda key_values: (key_values[0], sum(key_values[1]) / len(key_values[1])))\n",
    "\n",
    "\n",
    "counts.coalesce(1)\\\n",
    "      .sortBy(lambda a: a[0])\\\n",
    "      .saveAsTextFile(\"./average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 1.150943396226415)\n",
      "('2', 1.0769230769230769)\n",
      "('3', 1.0)\n",
      "('4', 1.5)\n",
      "('5', 1.5)\n",
      "('6', 1.75)\n",
      "('7', 1.0)\n",
      "('8', 1.6666666666666667)\n",
      "('9', 1.0)\n",
      "('A', 3.901754225255347)\n",
      "('B', 5.143532818532819)\n",
      "('C', 6.634214463840399)\n",
      "('D', 5.221781152916811)\n",
      "('E', 5.53018939875429)\n",
      "('F', 5.265583343912657)\n",
      "('G', 5.810282153366799)\n",
      "('H', 4.428398058252427)\n",
      "('I', 1.4687346778674861)\n",
      "('J', 4.9784550709406\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./average/part-00000\", \"r\")\n",
    "file_content = f.read()\n",
    "f.close()\n",
    "\n",
    "print(file_content[:390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.0.1/libexec\r\n"
     ]
    }
   ],
   "source": [
    "!echo $SPARK_HOME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
